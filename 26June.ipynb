{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 1, stride=1, padding=0)\n",
    "        for j in range(3):\n",
    "            for i in range(6):\n",
    "                exec(\"self.Resconv1\"+str(j)+str(i)+\"=\"+\"nn.Conv2d(32, 32, 3, stride=1, padding=1)\")\n",
    "                exec(\"self.Batch_norm1\"+str(j)+str(i)+\"=\"+\"nn.BatchNorm2d(32, track_running_stats=False)\")\n",
    "                exec(\"self.Resconv2\"+str(j)+str(i)+\"=\"+\"nn.Conv2d(32, 32, 3, stride=1, padding=1)\")\n",
    "                exec(\"self.Batch_norm2\"+str(j)+str(i)+\"=\"+\"nn.BatchNorm2d(32, track_running_stats=False)\")\n",
    "#         self.Resconv1 = nn.Conv2d(32, 32, 3, stride=1, padding=1)\n",
    "#         self.Batch_norm1 = nn.BatchNorm2d(32)\n",
    "#         self.Resconv2 = nn.Conv2d(32, 32, 3, stride=1, padding=1)\n",
    "#         self.Batch_norm2 = nn.BatchNorm2d(32)\n",
    "        self.Transconv1 = nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1, output_padding = 1)\n",
    "        self.Transconv2 = nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1, output_padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 3*256, 1, stride=1, padding=0)\n",
    "\n",
    "        self.Maskconv1 = nn.Conv2d(3, 64, 7, stride=1, padding=3)\n",
    "        for i in range(20):\n",
    "            exec(\"self.Gateconv1\"+str(i)+ \"=\"+ \"nn.Conv2d(64, 2*64, 5, stride=1, padding=2)\")\n",
    "            exec(\"self.Gateconv2\"+str(i)+ \"=\"+ \"nn.Conv2d(2*64, 2*64, (1,1), stride=1, padding=0)\")\n",
    "            exec(\"self.Gateconv3\"+str(i)+ \"=\"+ \"nn.Conv2d(64, 2*64, (1,5), stride=1, padding=(0,2))\")\n",
    "            exec(\"self.Gateconv4\"+str(i)+ \"=\"+ \"nn.Conv2d(64, 64, (1,1), stride=1, padding=0)\")\n",
    "        self.Maskconv2 = nn.Conv2d(64, 1024, 1, stride=1, padding=0)\n",
    "        self.Maskconv3 = nn.Conv2d(1024, 3*256, 1, stride=1, padding=0)\n",
    "\n",
    "#         self.conv1.weight = nn.Parameter(self.weight_init(3, 32, (1,1), mask_type = None))\n",
    "#         self.Resconv1.weight = nn.Parameter(self.weight_init(32, 32, (3,3), mask_type = None))\n",
    "#         self.Transconv1.weight = nn.Parameter(self.weight_init(32, 32, (3,3), mask_type = None))\n",
    "#         self.Resconv2.weight = nn.Parameter(self.weight_init(32, 32, (3,3), mask_type = None))\n",
    "#         self.Transconv2.weight = nn.Parameter(self.weight_init(32, 32, (3,3), mask_type = None))\n",
    "#         self.conv2.weight = nn.Parameter(self.weight_init(32, 3*256, (1,1), mask_type = None))\n",
    "\n",
    "        self.Maskconv1.weight = nn.Parameter(self.weight_init(3, 64, (7,7), mask_type='A'))\n",
    "        for i in range(20):\n",
    "            exec(\"self.Gateconv1\"+str(i)+\".weight\"+\"=\"+\"nn.Parameter(self.weight_init(64, 2*64, (5,5), mask_type='C'))\")\n",
    "            exec(\"self.Gateconv2\"+str(i)+\".weight\"+\"=\"+\"nn.Parameter(self.weight_init(2*64, 2*64, (1,1), mask_type=None))\")\n",
    "            exec(\"self.Gateconv3\"+str(i)+\".weight\"+\"=\"+ \"nn.Parameter(self.weight_init(64, 2*64, (1,5), mask_type='B'))\")\n",
    "            exec(\"self.Gateconv4\"+str(i)+\".weight\"+\"=\"+\"nn.Parameter(self.weight_init(64, 64, (1,1), mask_type='B'))\")\n",
    "        self.Maskconv2.weight = nn.Parameter(self.weight_init(64, 1024, (1,1), mask_type='B'))\n",
    "        self.Maskconv3.weight = nn.Parameter(self.weight_init(1024, 3*256, (1,1), mask_type='B'))\n",
    "\n",
    "    def weight_init(self, in_channel, num_outputs, kernel_shape, mask_type=None):\n",
    "        kernel_h, kernel_w = kernel_shape\n",
    "        center_h = kernel_h // 2\n",
    "        center_w = kernel_w // 2\n",
    "\n",
    "        mask = np.zeros((num_outputs, in_channel, kernel_h, kernel_w), dtype=np.float32)\n",
    "        if mask_type is not None:\n",
    "            mask[:, :, :center_h, :] = 1\n",
    "            if mask_type == 'A':\n",
    "                mask[:, :, center_h, :center_w] = 1\n",
    "            if mask_type == 'B':\n",
    "                mask[:, :, center_h, :center_w+1] = 1\n",
    "        else:\n",
    "            mask[:, :, :, :] = 1\n",
    "\n",
    "        weights_shape = [num_outputs, in_channel, kernel_h, kernel_w]\n",
    "        weights = torch.empty(weights_shape, requires_grad=True)\n",
    "        weights = nn.init.xavier_normal_(weights)\n",
    "        weights = weights*torch.from_numpy(mask)\n",
    "        return weights\n",
    "\n",
    "    def prior_network(self, hr_images):\n",
    "        conv1 = self.Maskconv1(hr_images)\n",
    "        inputs = conv1\n",
    "        state = conv1\n",
    "        for i in range(20):\n",
    "            inputs, state = self.gated_conv2d(inputs, state, [5, 5], i)\n",
    "        conv2 = self.Maskconv2(inputs)\n",
    "        conv2 = F.relu(conv2)\n",
    "        prior_logits = self.Maskconv3(conv2)\n",
    "        prior_logits = torch.cat((prior_logits[:, 0::3, :, :], prior_logits[:, 1::3, :, :], prior_logits[:, 2::3, :, :]), 1)\n",
    "        return prior_logits\n",
    "\n",
    "    def conditioning_network(self, lr_images):\n",
    "        res_num = 6\n",
    "        inputs = lr_images\n",
    "        inputs = self.conv1(inputs)\n",
    "        for i in range(2):\n",
    "            for j in range(res_num):\n",
    "                inputs = self.resnet_block(inputs, i, j)\n",
    "            inputs = eval(\"self.Transconv\"+str(i+1))(inputs)\n",
    "            inputs = F.relu(inputs)\n",
    "        for i in range(res_num):\n",
    "            inputs = self.resnet_block(inputs, 2, i)\n",
    "        conditioning_logits = self.conv2(inputs)\n",
    "        return conditioning_logits\n",
    "\n",
    "#     def batch_norm(self, x):\n",
    "#         bn = nn.BatchNorm2d(x.shape[1], affine=True, track_running_stats=True)\n",
    "#         return bn(x)\n",
    "\n",
    "    def resnet_block(self, inputs, i, j):\n",
    "        conv1 = eval(\"self.Resconv1\"+str(i)+str(j))(inputs)\n",
    "        bn1 = eval(\"self.Batch_norm1\"+str(i)+str(j))(conv1)\n",
    "        relu1 = F.relu(bn1)\n",
    "        conv2 = eval(\"self.Resconv2\"+str(i)+str(j))(relu1)\n",
    "        bn2 = eval(\"self.Batch_norm2\"+str(i)+str(j))(conv2)\n",
    "        output = inputs + bn2\n",
    "        return output\n",
    "\n",
    "\n",
    "    def gated_conv2d(self, inputs, state, kernel_shape, i):\n",
    "        batch_size, in_channel, height, width  = list(inputs.size())\n",
    "        kernel_h, kernel_w = kernel_shape\n",
    "        left = eval(\"self.Gateconv1\"+str(i))(state)\n",
    "        left1 = left[:, 0:in_channel, :, :]\n",
    "        left2 = left[:, in_channel:, :, :]\n",
    "        left1 = F.tanh(left1)\n",
    "        left2 = F.sigmoid(left2)\n",
    "        new_state = left1 * left2\n",
    "        left2right = eval(\"self.Gateconv2\"+str(i))(left)\n",
    "        right = eval(\"self.Gateconv3\"+str(i))(inputs)\n",
    "        right = right + left2right\n",
    "        right1 = right[:, 0:in_channel, :, :]\n",
    "        right2 = right[:, in_channel:, :, :]\n",
    "        right1 = F.tanh(right1)\n",
    "        right2 = F.sigmoid(right2)\n",
    "        up_right = right1 * right2\n",
    "        up_right = eval(\"self.Gateconv4\"+str(i))(up_right)\n",
    "        outputs = inputs + up_right\n",
    "        return outputs, new_state\n",
    "\n",
    "    def forward(self, lr_images, hr_images):\n",
    "        hr_images = hr_images - 0.5\n",
    "        lr_images = lr_images - 0.5\n",
    "        prior_logits = self.prior_network(hr_images)\n",
    "        conditioning_logits = self.conditioning_network(lr_images)\n",
    "        return prior_logits, conditioning_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(logits, labels):\n",
    "    logits = torch.reshape(logits, [-1, 256])\n",
    "    labels = labels.to(torch.int64)\n",
    "    labels = torch.reshape(labels, [-1])\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "def train(model, device, train_loader_lr, train_loader_hr, optimizer, epoch):\n",
    "    model.train()\n",
    "#     for name, param in model.named_parameters():\n",
    "#                 if param.requires_grad:\n",
    "#                     print(name, param.data)\n",
    "    for (batch_idx, (data, t)), (target, p) in zip(enumerate(train_loader_lr), train_loader_hr):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        prior_logits, conditioning_logits = model(lr_images = data, hr_images = target)\n",
    "        l1 = softmax_loss(prior_logits+conditioning_logits, torch.floor(target*255))\n",
    "        l2 = softmax_loss(conditioning_logits, torch.floor(target*255))\n",
    "        loss = l1+l2\n",
    "        loss3 = softmax_loss(prior_logits, torch.floor(target*255))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 1 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), (len(train_loader_lr)*len(data)),\n",
    "                100. * batch_idx / len(train_loader_lr), loss.item()))\n",
    "\n",
    "        if batch_idx % 100 == 0 and batch_idx != 0:\n",
    "#             for name, param in model.named_parameters():\n",
    "#                 if param.requires_grad:\n",
    "#                     print(name, param.data)\n",
    "            sample(model, data, target, len(data), mu=1.1, step=epoch*batch_idx)\n",
    "\n",
    "\n",
    "\n",
    "def test(model, device, test_loader_lr, test_loader_hr):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (data, t), (target, p) in zip(test_loader_lr, test_loader_hr):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            prior_logits, conditioning_logits = model(lr_images = data, hr_images = target)\n",
    "            l1 = softmax_loss(prior_logits+conditioning_logits, torch.floor(target*255))\n",
    "            l2 = softmax_loss(conditioning_logits, torch.floor(target*255))\n",
    "            test_loss += l1+l2 # sum up batch loss\n",
    "\n",
    "    test_loss /= len(test_loader_lr)*32\n",
    "    print(\"test_loss : \", test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_2_pixel_value(logits, mu=1.1):\n",
    "    # print(\"convert\")\n",
    "#     print(\"logits\", logits)\n",
    "    rebalance_logits = logits * mu\n",
    "    probs = softmax(rebalance_logits)\n",
    "    pixel_dict = torch.arange(0, 256, dtype=torch.float32)\n",
    "    pixels = torch.sum(probs*pixel_dict, dim=1)\n",
    "    return pixels/255\n",
    "\n",
    "def softmax(x):\n",
    "    # print(\"cal\")\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # x = x.detach().numpy()\n",
    "    a, b = torch.max(x, -1, keepdim=True, out=None)\n",
    "    e_x = torch.exp(x - a)\n",
    "    return e_x / e_x.sum(dim=-1, keepdim =True) # only difference\n",
    "\n",
    "def sample(model, data, target, batch_size, mu=1.1, step=None):\n",
    "    with torch.no_grad():\n",
    "        np_lr_imgs = data\n",
    "        np_hr_imgs = target\n",
    "        c_logits = model.conditioning_network\n",
    "        p_logits = model.prior_network\n",
    "        gen_hr_imgs = torch.zeros((batch_size, 3, 32, 32), dtype=torch.float32)\n",
    "        np_c_logits = c_logits(torch.floor(np_lr_imgs*255))\n",
    "        print(\"torch.floor(np_lr_imgs*255)\", torch.floor(np_lr_imgs*255)[0])\n",
    "        print(\"c_logits\", np_c_logits[0].shape, np_c_logits[0])\n",
    "        print(\"p_logits\", p_logits(gen_hr_imgs)[0].shape, p_logits(gen_hr_imgs)[0])\n",
    "        print(\"target\", torch.floor(target*255)[0])\n",
    "        for i in range(32):\n",
    "            print(i)\n",
    "            for j in range(32):\n",
    "                for c in range(3):\n",
    "#                     print(i,j,c)\n",
    "                    np_p_logits = p_logits(gen_hr_imgs)\n",
    "                    new_pixel = logits_2_pixel_value(np_c_logits[:, c*256:(c+1)*256, i, j] + np_p_logits[:, c*256:(c+1)*256, i, j], mu=mu)\n",
    "                    gen_hr_imgs[:, c, i, j] = new_pixel\n",
    "                    # try:\n",
    "                    # \tfor obj in gc.get_objects():\n",
    "                    # \t\tif torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                    # \t\t\tprint(type(obj), obj.size())\n",
    "                    # except:\n",
    "                    # \tprint(\"error\")\n",
    "        samples_dir = \"/home/eee/ug/15084005/DIH/samples/\"\n",
    "        print(\"sample\")\n",
    "        save_samples(np_lr_imgs, samples_dir + '/lr_' + str(mu*10) + '_' + str(step))\n",
    "        save_samples(np_hr_imgs, samples_dir + '/hr_' + str(mu*10) + '_' + str(step))\n",
    "        save_samples(gen_hr_imgs, samples_dir + '/generate_' + str(mu*10) + '_' + str(step))\n",
    "\n",
    "def save_samples(np_imgs, img_path):\n",
    "    print(\"save\")\n",
    "    torchvision.utils.save_image(np_imgs[0, :, :, :], img_path+\".jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--use_gpu\", type = bool, default = True, help = \"use or not gpu\")\n",
    "#     parser.add_argument(\"--num_epoch\", type = int, default = 30, help = \"no of epoch\")\n",
    "#     parser.add_argument(\"--batch_size\", type = int, default = 32, help = \"batch size\")\n",
    "#     parser.add_argument(\"--learning_rate\", type = float, default = 4e-4, help = \"learning rate\")\n",
    "#     args = parser.parse_args()\n",
    "    use_gpu = False\n",
    "    num_epoch = 30\n",
    "    batch_size = 32\n",
    "    learning_rate = 4e-4\n",
    "\n",
    "    use_cuda = use_gpu and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "#     torch.cuda.set_device(0)\n",
    "\n",
    "    data = datasets.ImageFolder(root = '/home/eee/ug/15084005/DIH/CelebA/CelebA/train/', transform = transforms.Compose([transforms.Resize((8,8), interpolation=2), transforms.ToTensor()]))\n",
    "    target = datasets.ImageFolder(root = '/home/eee/ug/15084005/DIH/CelebA/CelebA/train/', transform = transforms.Compose([transforms.Resize((32,32), interpolation=2), transforms.ToTensor()]))\n",
    "\n",
    "    data_test = datasets.ImageFolder(root = '/home/eee/ug/15084005/DIH/CelebA/CelebA/test/', transform = transforms.Compose([transforms.Resize((8,8), interpolation=2), transforms.ToTensor()]))\n",
    "    target_test = datasets.ImageFolder(root = '/home/eee/ug/15084005/DIH/CelebA/CelebA/test/', transform = transforms.Compose([transforms.Resize((32,32), interpolation=2), transforms.ToTensor()]))\n",
    "\n",
    "\n",
    "    #test_split = .05\n",
    "    #shuffle_dataset = True\n",
    "    #random_seed= 42\n",
    "    # Creating data indices for training and validation splits:\n",
    "    # dataset_size = len(image_data_hr)\n",
    "    # indices = list(range(dataset_size))\n",
    "    # split = int(np.floor(test_split * dataset_size))\n",
    "    # if shuffle_dataset :\n",
    "    # \tnp.random.seed(random_seed)\n",
    "    # \tnp.random.shuffle(indices)\n",
    "    # train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # data = [image_data_lr[x] for x in train_indices]\n",
    "    # target = [image_data_hr[x] for x in train_indices]\n",
    "\n",
    "    # data_test = [image_data_lr[x] for x in train_indices]\n",
    "    # target_test = [image_data_hr[x] for x in train_indices]\n",
    "\n",
    "    train_sampler_lr = torch.utils.data.sampler.SequentialSampler(data)\n",
    "    train_sampler_hr = torch.utils.data.sampler.SequentialSampler(target)\n",
    "\n",
    "    test_sampler_lr = torch.utils.data.sampler.SequentialSampler(data_test)\n",
    "    test_sampler_hr = torch.utils.data.sampler.SequentialSampler(target_test)\n",
    "\n",
    "    train_loader_lr = torch.utils.data.DataLoader(data, batch_size=batch_size, sampler=train_sampler_lr)\n",
    "    train_loader_hr = torch.utils.data.DataLoader(target, batch_size=batch_size, sampler=train_sampler_hr)\n",
    "\n",
    "    test_loader_lr = torch.utils.data.DataLoader(data_test, batch_size=batch_size, sampler=test_sampler_lr)\n",
    "    test_loader_hr = torch.utils.data.DataLoader(target_test, batch_size=batch_size, sampler=test_sampler_hr)\n",
    "\n",
    "    model = Net()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.95)\n",
    "    print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "#     for parameter in model.parameters():\n",
    "#         print(parameter)\n",
    "#     print(list(model.Maskconv1.parameters()))\n",
    "#     print(list(model.Gateconv4.parameters()))\n",
    "    for epoch in range(1, num_epoch+1):\n",
    "        train(model, device, train_loader_lr, train_loader_hr, optimizer, epoch)\n",
    "        # sample(model, train_loader_lr, train_loader_hr, mu=1.1, step=epoch)\n",
    "        test(model, device, test_loader_lr, test_loader_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6576192\n",
      "Train Epoch: 1 [0/192608 (0%)]\tLoss: 11.732662\n",
      "Train Epoch: 1 [32/192608 (0%)]\tLoss: 11.636530\n",
      "Train Epoch: 1 [64/192608 (0%)]\tLoss: 11.418681\n",
      "Train Epoch: 1 [96/192608 (0%)]\tLoss: 11.248533\n",
      "Train Epoch: 1 [128/192608 (0%)]\tLoss: 11.146524\n",
      "Train Epoch: 1 [160/192608 (0%)]\tLoss: 11.119799\n",
      "Train Epoch: 1 [192/192608 (0%)]\tLoss: 11.122786\n",
      "Train Epoch: 1 [224/192608 (0%)]\tLoss: 11.146030\n",
      "Train Epoch: 1 [256/192608 (0%)]\tLoss: 11.114670\n",
      "Train Epoch: 1 [288/192608 (0%)]\tLoss: 11.110754\n",
      "Train Epoch: 1 [320/192608 (0%)]\tLoss: 11.105581\n",
      "Train Epoch: 1 [352/192608 (0%)]\tLoss: 11.100151\n",
      "Train Epoch: 1 [384/192608 (0%)]\tLoss: 11.099194\n",
      "Train Epoch: 1 [416/192608 (0%)]\tLoss: 11.097786\n",
      "Train Epoch: 1 [448/192608 (0%)]\tLoss: 11.097876\n",
      "Train Epoch: 1 [480/192608 (0%)]\tLoss: 11.097520\n",
      "Train Epoch: 1 [512/192608 (0%)]\tLoss: 11.095774\n",
      "Train Epoch: 1 [544/192608 (0%)]\tLoss: 11.096651\n",
      "Train Epoch: 1 [576/192608 (0%)]\tLoss: 11.096519\n",
      "Train Epoch: 1 [608/192608 (0%)]\tLoss: 11.095449\n",
      "Train Epoch: 1 [640/192608 (0%)]\tLoss: 11.095777\n",
      "Train Epoch: 1 [672/192608 (0%)]\tLoss: 11.095652\n",
      "Train Epoch: 1 [704/192608 (0%)]\tLoss: 11.094353\n",
      "Train Epoch: 1 [736/192608 (0%)]\tLoss: 11.093612\n",
      "Train Epoch: 1 [768/192608 (0%)]\tLoss: 11.095300\n",
      "Train Epoch: 1 [800/192608 (0%)]\tLoss: 11.095304\n",
      "Train Epoch: 1 [832/192608 (0%)]\tLoss: 11.094517\n",
      "Train Epoch: 1 [864/192608 (0%)]\tLoss: 11.093887\n",
      "Train Epoch: 1 [896/192608 (0%)]\tLoss: 11.094435\n",
      "Train Epoch: 1 [928/192608 (0%)]\tLoss: 11.093468\n",
      "Train Epoch: 1 [960/192608 (0%)]\tLoss: 11.091506\n",
      "Train Epoch: 1 [992/192608 (1%)]\tLoss: 11.089823\n",
      "Train Epoch: 1 [1024/192608 (1%)]\tLoss: 11.089565\n",
      "Train Epoch: 1 [1056/192608 (1%)]\tLoss: 11.091862\n",
      "Train Epoch: 1 [1088/192608 (1%)]\tLoss: 11.090915\n",
      "Train Epoch: 1 [1120/192608 (1%)]\tLoss: 11.088395\n",
      "Train Epoch: 1 [1152/192608 (1%)]\tLoss: 11.087626\n",
      "Train Epoch: 1 [1184/192608 (1%)]\tLoss: 11.085306\n",
      "Train Epoch: 1 [1216/192608 (1%)]\tLoss: 11.085930\n",
      "Train Epoch: 1 [1248/192608 (1%)]\tLoss: 11.086794\n",
      "Train Epoch: 1 [1280/192608 (1%)]\tLoss: 11.082415\n",
      "Train Epoch: 1 [1312/192608 (1%)]\tLoss: 11.084432\n",
      "Train Epoch: 1 [1344/192608 (1%)]\tLoss: 11.082465\n",
      "Train Epoch: 1 [1376/192608 (1%)]\tLoss: 11.082238\n",
      "Train Epoch: 1 [1408/192608 (1%)]\tLoss: 11.084627\n",
      "Train Epoch: 1 [1440/192608 (1%)]\tLoss: 11.088732\n",
      "Train Epoch: 1 [1472/192608 (1%)]\tLoss: 11.085546\n",
      "Train Epoch: 1 [1504/192608 (1%)]\tLoss: 11.084635\n",
      "Train Epoch: 1 [1536/192608 (1%)]\tLoss: 11.081999\n",
      "Train Epoch: 1 [1568/192608 (1%)]\tLoss: 11.084820\n",
      "Train Epoch: 1 [1600/192608 (1%)]\tLoss: 11.079074\n",
      "Train Epoch: 1 [1632/192608 (1%)]\tLoss: 11.079592\n",
      "Train Epoch: 1 [1664/192608 (1%)]\tLoss: 11.083946\n",
      "Train Epoch: 1 [1696/192608 (1%)]\tLoss: 11.082502\n",
      "Train Epoch: 1 [1728/192608 (1%)]\tLoss: 11.076931\n",
      "Train Epoch: 1 [1760/192608 (1%)]\tLoss: 11.080788\n",
      "Train Epoch: 1 [1792/192608 (1%)]\tLoss: 11.081369\n",
      "Train Epoch: 1 [1824/192608 (1%)]\tLoss: 11.081716\n",
      "Train Epoch: 1 [1856/192608 (1%)]\tLoss: 11.081370\n",
      "Train Epoch: 1 [1888/192608 (1%)]\tLoss: 11.080884\n",
      "Train Epoch: 1 [1920/192608 (1%)]\tLoss: 11.080341\n",
      "Train Epoch: 1 [1952/192608 (1%)]\tLoss: 11.082730\n",
      "Train Epoch: 1 [1984/192608 (1%)]\tLoss: 11.083034\n",
      "Train Epoch: 1 [2016/192608 (1%)]\tLoss: 11.080288\n",
      "Train Epoch: 1 [2048/192608 (1%)]\tLoss: 11.080025\n",
      "Train Epoch: 1 [2080/192608 (1%)]\tLoss: 11.081456\n",
      "Train Epoch: 1 [2112/192608 (1%)]\tLoss: 11.082687\n",
      "Train Epoch: 1 [2144/192608 (1%)]\tLoss: 11.080173\n",
      "Train Epoch: 1 [2176/192608 (1%)]\tLoss: 11.080070\n",
      "Train Epoch: 1 [2208/192608 (1%)]\tLoss: 11.079502\n",
      "Train Epoch: 1 [2240/192608 (1%)]\tLoss: 11.080102\n",
      "Train Epoch: 1 [2272/192608 (1%)]\tLoss: 11.082343\n",
      "Train Epoch: 1 [2304/192608 (1%)]\tLoss: 11.082460\n",
      "Train Epoch: 1 [2336/192608 (1%)]\tLoss: 11.080859\n",
      "Train Epoch: 1 [2368/192608 (1%)]\tLoss: 11.080177\n",
      "Train Epoch: 1 [2400/192608 (1%)]\tLoss: 11.079715\n",
      "Train Epoch: 1 [2432/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2464/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2496/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2528/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2560/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2592/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2624/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2656/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2688/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2720/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2752/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2784/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2816/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2848/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2880/192608 (1%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2912/192608 (2%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2944/192608 (2%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [2976/192608 (2%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [3008/192608 (2%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [3040/192608 (2%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [3072/192608 (2%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [3104/192608 (2%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [3136/192608 (2%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [3168/192608 (2%)]\tLoss: 11.079620\n",
      "Train Epoch: 1 [3200/192608 (2%)]\tLoss: 11.079620\n",
      "torch.floor(np_lr_imgs*255) tensor([[[  10.,    6.,    1.,    0.,    1.,    1.,    1.,    1.],\n",
      "         [  89.,   68.,   19.,   14.,   10.,    2.,    1.,    1.],\n",
      "         [  64.,   62.,  115.,  139.,  107.,   76.,   58.,   40.],\n",
      "         [  48.,   80.,  182.,  188.,  133.,   32.,   42.,   61.],\n",
      "         [  41.,  111.,  191.,  195.,  161.,   42.,    2.,    2.],\n",
      "         [  51.,  100.,  160.,  155.,  116.,   34.,    1.,    1.],\n",
      "         [  22.,   77.,  108.,  131.,   78.,    9.,    1.,    1.],\n",
      "         [  20.,   94.,   99.,   70.,   34.,   33.,   33.,   14.]],\n",
      "\n",
      "        [[   4.,    7.,   16.,   17.,   17.,   18.,   18.,   18.],\n",
      "         [  37.,   32.,   24.,   22.,   19.,   18.,   18.,   18.],\n",
      "         [  56.,   49.,   85.,   95.,   54.,   34.,   29.,   26.],\n",
      "         [  48.,   62.,  128.,  129.,   89.,   28.,   27.,   31.],\n",
      "         [  38.,   71.,  128.,  138.,  107.,   40.,   20.,   21.],\n",
      "         [  40.,   57.,   93.,  100.,   74.,   33.,   20.,   20.],\n",
      "         [  27.,   46.,   60.,   77.,   50.,   22.,   20.,   19.],\n",
      "         [  28.,   55.,   53.,   42.,   32.,   32.,   31.,   24.]],\n",
      "\n",
      "        [[   7.,   14.,   24.,   25.,   25.,   26.,   25.,   25.],\n",
      "         [  30.,   30.,   28.,   29.,   26.,   26.,   25.,   25.],\n",
      "         [  50.,   44.,   60.,   65.,   43.,   33.,   30.,   29.],\n",
      "         [  44.,   48.,   89.,   91.,   66.,   31.,   30.,   32.],\n",
      "         [  36.,   50.,   90.,  100.,   77.,   38.,   28.,   28.],\n",
      "         [  36.,   41.,   57.,   70.,   62.,   34.,   27.,   27.],\n",
      "         [  31.,   37.,   43.,   54.,   44.,   29.,   27.,   27.],\n",
      "         [  33.,   41.,   40.,   37.,   34.,   32.,   32.,   29.]]])\n",
      "c_logits torch.Size([768, 32, 32]) tensor(1.00000e-04 *\n",
      "       [[[ 5.2752,  5.2754,  5.2753,  ...,  5.2753,  5.2754,  5.2754],\n",
      "         [ 5.2752,  5.2754,  5.2754,  ...,  5.2754,  5.2754,  5.2755],\n",
      "         [ 5.2752,  5.2754,  5.2754,  ...,  5.2754,  5.2754,  5.2755],\n",
      "         ...,\n",
      "         [ 5.2752,  5.2754,  5.2753,  ...,  5.2754,  5.2754,  5.2755],\n",
      "         [ 5.2752,  5.2753,  5.2753,  ...,  5.2754,  5.2754,  5.2754],\n",
      "         [ 5.2754,  5.2755,  5.2755,  ...,  5.2755,  5.2755,  5.2755]],\n",
      "\n",
      "        [[-1.0710, -1.0709, -1.0709,  ..., -1.0709, -1.0708, -1.0709],\n",
      "         [-1.0709, -1.0708, -1.0708,  ..., -1.0708, -1.0708, -1.0709],\n",
      "         [-1.0709, -1.0708, -1.0708,  ..., -1.0708, -1.0708, -1.0709],\n",
      "         ...,\n",
      "         [-1.0709, -1.0708, -1.0708,  ..., -1.0708, -1.0708, -1.0709],\n",
      "         [-1.0709, -1.0708, -1.0708,  ..., -1.0708, -1.0708, -1.0709],\n",
      "         [-1.0710, -1.0709, -1.0709,  ..., -1.0709, -1.0708, -1.0709]],\n",
      "\n",
      "        [[-2.8265, -2.8263, -2.8263,  ..., -2.8263, -2.8263, -2.8265],\n",
      "         [-2.8263, -2.8260, -2.8260,  ..., -2.8260, -2.8260, -2.8263],\n",
      "         [-2.8263, -2.8261, -2.8260,  ..., -2.8261, -2.8260, -2.8263],\n",
      "         ...,\n",
      "         [-2.8263, -2.8261, -2.8261,  ..., -2.8261, -2.8260, -2.8263],\n",
      "         [-2.8263, -2.8261, -2.8261,  ..., -2.8261, -2.8261, -2.8263],\n",
      "         [-2.8262, -2.8260, -2.8260,  ..., -2.8260, -2.8260, -2.8262]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.0208,  2.0209,  2.0209,  ...,  2.0209,  2.0209,  2.0206],\n",
      "         [ 2.0210,  2.0210,  2.0210,  ...,  2.0210,  2.0210,  2.0204],\n",
      "         [ 2.0210,  2.0210,  2.0210,  ...,  2.0210,  2.0210,  2.0204],\n",
      "         ...,\n",
      "         [ 2.0210,  2.0210,  2.0210,  ...,  2.0210,  2.0210,  2.0204],\n",
      "         [ 2.0210,  2.0210,  2.0210,  ...,  2.0210,  2.0210,  2.0203],\n",
      "         [ 2.0209,  2.0208,  2.0208,  ...,  2.0208,  2.0208,  2.0204]],\n",
      "\n",
      "        [[ 6.8554,  6.8552,  6.8552,  ...,  6.8552,  6.8552,  6.8552],\n",
      "         [ 6.8553,  6.8551,  6.8551,  ...,  6.8551,  6.8551,  6.8551],\n",
      "         [ 6.8553,  6.8551,  6.8551,  ...,  6.8551,  6.8551,  6.8551],\n",
      "         ...,\n",
      "         [ 6.8553,  6.8551,  6.8551,  ...,  6.8551,  6.8551,  6.8551],\n",
      "         [ 6.8553,  6.8551,  6.8551,  ...,  6.8551,  6.8551,  6.8551],\n",
      "         [ 6.8552,  6.8552,  6.8552,  ...,  6.8551,  6.8551,  6.8552]],\n",
      "\n",
      "        [[-5.8964, -5.8966, -5.8966,  ..., -5.8966, -5.8966, -5.8966],\n",
      "         [-5.8964, -5.8965, -5.8965,  ..., -5.8965, -5.8965, -5.8966],\n",
      "         [-5.8964, -5.8965, -5.8965,  ..., -5.8965, -5.8965, -5.8966],\n",
      "         ...,\n",
      "         [-5.8964, -5.8965, -5.8965,  ..., -5.8965, -5.8965, -5.8966],\n",
      "         [-5.8964, -5.8965, -5.8965,  ..., -5.8965, -5.8964, -5.8966],\n",
      "         [-5.8964, -5.8966, -5.8965,  ..., -5.8966, -5.8965, -5.8967]]])\n",
      "p_logits torch.Size([768, 32, 32]) tensor(1.00000e-04 *\n",
      "       [[[ 0.3030,  0.3030,  0.3030,  ...,  0.3030,  0.3030,  0.3030],\n",
      "         [ 0.3030,  0.3030,  0.3030,  ...,  0.3030,  0.3030,  0.3030],\n",
      "         [ 0.3030,  0.3030,  0.3030,  ...,  0.3030,  0.3030,  0.3030],\n",
      "         ...,\n",
      "         [ 0.3030,  0.3030,  0.3030,  ...,  0.3030,  0.3030,  0.3030],\n",
      "         [ 0.3030,  0.3030,  0.3030,  ...,  0.3030,  0.3030,  0.3030],\n",
      "         [ 0.3030,  0.3030,  0.3030,  ...,  0.3030,  0.3030,  0.3030]],\n",
      "\n",
      "        [[-1.3313, -1.3313, -1.3313,  ..., -1.3313, -1.3313, -1.3313],\n",
      "         [-1.3313, -1.3313, -1.3313,  ..., -1.3313, -1.3313, -1.3313],\n",
      "         [-1.3313, -1.3313, -1.3313,  ..., -1.3313, -1.3313, -1.3313],\n",
      "         ...,\n",
      "         [-1.3313, -1.3313, -1.3313,  ..., -1.3313, -1.3313, -1.3313],\n",
      "         [-1.3313, -1.3313, -1.3313,  ..., -1.3313, -1.3313, -1.3313],\n",
      "         [-1.3313, -1.3313, -1.3313,  ..., -1.3313, -1.3313, -1.3313]],\n",
      "\n",
      "        [[-0.0433, -0.0433, -0.0433,  ..., -0.0433, -0.0433, -0.0433],\n",
      "         [-0.0433, -0.0433, -0.0433,  ..., -0.0433, -0.0433, -0.0433],\n",
      "         [-0.0433, -0.0433, -0.0433,  ..., -0.0433, -0.0433, -0.0433],\n",
      "         ...,\n",
      "         [-0.0433, -0.0433, -0.0433,  ..., -0.0433, -0.0433, -0.0433],\n",
      "         [-0.0433, -0.0433, -0.0433,  ..., -0.0433, -0.0433, -0.0433],\n",
      "         [-0.0433, -0.0433, -0.0433,  ..., -0.0433, -0.0433, -0.0433]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0403,  0.0403,  0.0403,  ...,  0.0403,  0.0403,  0.0403],\n",
      "         [ 0.0403,  0.0403,  0.0403,  ...,  0.0403,  0.0403,  0.0403],\n",
      "         [ 0.0403,  0.0403,  0.0403,  ...,  0.0403,  0.0403,  0.0403],\n",
      "         ...,\n",
      "         [ 0.0403,  0.0403,  0.0403,  ...,  0.0403,  0.0403,  0.0403],\n",
      "         [ 0.0403,  0.0403,  0.0403,  ...,  0.0403,  0.0403,  0.0403],\n",
      "         [ 0.0403,  0.0403,  0.0403,  ...,  0.0403,  0.0403,  0.0403]],\n",
      "\n",
      "        [[-0.2556, -0.2556, -0.2556,  ..., -0.2556, -0.2556, -0.2556],\n",
      "         [-0.2556, -0.2556, -0.2556,  ..., -0.2556, -0.2556, -0.2556],\n",
      "         [-0.2556, -0.2556, -0.2556,  ..., -0.2556, -0.2556, -0.2556],\n",
      "         ...,\n",
      "         [-0.2556, -0.2556, -0.2556,  ..., -0.2556, -0.2556, -0.2556],\n",
      "         [-0.2556, -0.2556, -0.2556,  ..., -0.2556, -0.2556, -0.2556],\n",
      "         [-0.2556, -0.2556, -0.2556,  ..., -0.2556, -0.2556, -0.2556]],\n",
      "\n",
      "        [[-0.6024, -0.6024, -0.6024,  ..., -0.6024, -0.6024, -0.6024],\n",
      "         [-0.6024, -0.6024, -0.6024,  ..., -0.6024, -0.6024, -0.6024],\n",
      "         [-0.6024, -0.6024, -0.6024,  ..., -0.6024, -0.6024, -0.6024],\n",
      "         ...,\n",
      "         [-0.6024, -0.6024, -0.6024,  ..., -0.6024, -0.6024, -0.6024],\n",
      "         [-0.6024, -0.6024, -0.6024,  ..., -0.6024, -0.6024, -0.6024],\n",
      "         [-0.6024, -0.6024, -0.6024,  ..., -0.6024, -0.6024, -0.6024]]])\n",
      "target tensor([[[   0.,    1.,    3.,  ...,    1.,    2.,    1.],\n",
      "         [   0.,    1.,    3.,  ...,    1.,    1.,    1.],\n",
      "         [   3.,    2.,    3.,  ...,    1.,    1.,    1.],\n",
      "         ...,\n",
      "         [  15.,    8.,    7.,  ...,    4.,    3.,    2.],\n",
      "         [  21.,   14.,   13.,  ...,   11.,    4.,    3.],\n",
      "         [  25.,   17.,   17.,  ...,   59.,   26.,    5.]],\n",
      "\n",
      "        [[   6.,    2.,    0.,  ...,   17.,   19.,   18.],\n",
      "         [   7.,    3.,    0.,  ...,   17.,   18.,   18.],\n",
      "         [   6.,    2.,    0.,  ...,   17.,   18.,   18.],\n",
      "         ...,\n",
      "         [  26.,   23.,   23.,  ...,   21.,   20.,   19.],\n",
      "         [  31.,   26.,   28.,  ...,   22.,   20.,   21.],\n",
      "         [  33.,   26.,   31.,  ...,   38.,   26.,   20.]],\n",
      "\n",
      "        [[   6.,    1.,    1.,  ...,   24.,   26.,   25.],\n",
      "         [   7.,    2.,    1.,  ...,   24.,   25.,   25.],\n",
      "         [   9.,    4.,    2.,  ...,   24.,   25.,   25.],\n",
      "         ...,\n",
      "         [  35.,   30.,   31.,  ...,   29.,   28.,   27.],\n",
      "         [  39.,   34.,   36.,  ...,   27.,   25.,   27.],\n",
      "         [  39.,   35.,   40.,  ...,   32.,   27.,   26.]]])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-6a3819d07a95>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m#     print(list(model.Gateconv4.parameters()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_hr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;31m# sample(model, train_loader_lr, train_loader_hr, mu=1.1, step=epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_hr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5b36859e8656>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader_lr, train_loader_hr, optimizer, epoch)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#                 if param.requires_grad:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#                     print(name, param.data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c2d16886735a>\u001b[0m in \u001b[0;36msample\u001b[0;34m(model, data, target, batch_size, mu, step)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#                     print(i,j,c)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0mnp_p_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_hr_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                     \u001b[0mnew_pixel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_2_pixel_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_c_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp_p_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mgen_hr_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_pixel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fe8071c909b5>\u001b[0m in \u001b[0;36mprior_network\u001b[0;34m(self, hr_images)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgated_conv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fe8071c909b5>\u001b[0m in \u001b[0;36mgated_conv2d\u001b[0;34m(self, inputs, state, kernel_shape, i)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mkernel_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self.Gateconv1\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mleft1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0min_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mleft2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
